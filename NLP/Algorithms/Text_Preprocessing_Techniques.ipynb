{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Text Preprocessing with NLTK: A Beginner's Guide\n",
    "\n",
    "Welcome to this comprehensive guide on text preprocessing with NLTK (Natural Language Toolkit)! This notebook will walk you through various essential text preprocessing techniques, all explained in simple terms with easy-to-follow code examples. Whether you're just starting out in NLP (Natural Language Processing) or looking to brush up on your skills, you're in the right place! üöÄ\n",
    "\n",
    "\n",
    "NLTK provides a comprehensive suite of tools for processing and analyzing unstructured text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization \n",
    "Tokenization is the process of splitting text into individual words or sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello World.', 'This is NLTK.', 'It is great for text processing.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/praneet/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Hello World. This is NLTK. It is great for text processing.\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', '.', 'This', 'is', 'NLTK', '.', 'It', 'is', 'great', 'for', 'text', 'processing', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(text)\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Removing Stop Words\n",
    "Stop words are common words that may not be useful for text analysis (e.g., \"is\", \"the\", \"and\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', '.', 'NLTK', '.', 'great', 'text', 'processing', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/praneet/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "print(filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stemming\n",
    "Stemming reduces words to their root form by chopping off the ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', '.', 'nltk', '.', 'great', 'text', 'process', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Lemmatization\n",
    "Lemmatization reduces words to their base form (lemma), taking into account the meaning of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/praneet/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', '.', 'NLTK', '.', 'great', 'text', 'processing', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Part-of-Speech Tagging\n",
    "Tagging words with their parts of speech (POS) helps understand the grammatical structure.\n",
    "\n",
    "The complete POS tag list can be accessed from the Installation and set-up notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('World', 'NNP'), ('.', '.'), ('NLTK', 'NNP'), ('.', '.'), ('great', 'JJ'), ('text', 'JJ'), ('processing', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/praneet/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "pos_tags = nltk.pos_tag(lemmatized_words)\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Named Entity Recognition\n",
    "Identify named entities such as names of people, organizations, locations, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/praneet/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/praneet/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Hello/NNP)\n",
      "  World/NNP\n",
      "  ./.\n",
      "  NLTK/NNP\n",
      "  ./.\n",
      "  great/JJ\n",
      "  text/JJ\n",
      "  processing/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Numpy is required to run this\n",
    "%pip install numpy\n",
    "\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "print(named_entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Word Frequency Distribution\n",
    "Count the frequency of each word in the text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 3), ('Hello', 1), ('World', 1), ('NLTK', 1), ('great', 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "freq_dist = FreqDist(lemmatized_words)\n",
    "print(freq_dist.most_common(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Removing Punctuation\n",
    "Remove punctuation from the text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', 'NLTK', 'great', 'text', 'processing']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "no_punct = [word for word in lemmatized_words if word not in string.punctuation]\n",
    "print(no_punct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Lowercasing\n",
    "Convert all words to lowercase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'nltk', 'great', 'text', 'processing']\n"
     ]
    }
   ],
   "source": [
    "lowercased = [word.lower() for word in no_punct]\n",
    "print(lowercased)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Spelling Correction\n",
    "Correct the spelling of words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in /Users/praneet/anaconda3/envs/proj1maverick/lib/python3.9/site-packages (0.8.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "['hello', 'world', '.', 'known', 'text', 'process', '.']\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspellchecker\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "def correct_spelling(word):\n",
    "    if not wordnet.synsets(word):\n",
    "        return spell.correction(word)\n",
    "    return word\n",
    "\n",
    "lemmatized_words = ['hello', 'world', '.', 'klown', 'taxt', 'procass', '.']\n",
    "words_with_corrected_spelling = [correct_spelling(word) for word in lemmatized_words]\n",
    "print(words_with_corrected_spelling)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Removing Numbers\n",
    "Remove numerical values from the text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'text', 'process', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_words = ['hello', 'world', '88', 'text', 'process', '.']\n",
    "\n",
    "no_numbers = [word for word in lemmatized_words if not word.isdigit()]\n",
    "print(no_numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Word Replacement\n",
    "Replace specific words with other words (e.g., replacing slang with formal words).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'great', 'text', 'Natural Language Toolkit', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_words = ['hello', 'world', 'gr8', 'text', 'NLTK', '.']\n",
    "replacements = {'NLTK': 'Natural Language Toolkit', 'gr8' : 'great'}\n",
    "\n",
    "replaced_words = [replacements.get(word, word) for word in lemmatized_words]\n",
    "print(replaced_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Synonym Replacement\n",
    "Replace words with their synonyms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'universe', 'amazing', 'text', 'great', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "lemmatized_words = ['hello', 'world', 'awesome', 'text', 'great', '.']\n",
    "\n",
    "def get_synonym(word):\n",
    "    synonyms = wordnet.synsets(word)\n",
    "    if synonyms:\n",
    "        return synonyms[0].lemmas()[0].name()\n",
    "    return word\n",
    "\n",
    "synonym_replaced = [get_synonym(word) for word in lemmatized_words]\n",
    "print(synonym_replaced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Extracting Bigrams and Trigrams\n",
    "Extract bigrams (pairs of consecutive words) and trigrams (triplets of consecutive words).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 'world'), ('world', 'awesome'), ('awesome', 'text'), ('text', 'great'), ('great', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "bigrams_list = list(bigrams(lemmatized_words))\n",
    "print(bigrams_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 'world', 'awesome'), ('world', 'awesome', 'text'), ('awesome', 'text', 'great'), ('text', 'great', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import trigrams\n",
    "\n",
    "trigrams_list = list(trigrams(lemmatized_words))\n",
    "print(trigrams_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Sentence Segmentation\n",
    "Split text into sentences while considering abbreviations and other punctuation complexities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello World.', 'This is NLTK.', 'It is great for text preprocessing.']\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "\n",
    "text = 'Hello World. This is NLTK. It is great for text preprocessing.'\n",
    "\n",
    "# Load the sentence tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = tokenizer.tokenize(text)\n",
    "\n",
    "# Print the tokenized sentences\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Identifying Word Frequencies\n",
    "Identify and display the frequency of words in a text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello: 2\n",
      "awesome: 1\n",
      "text: 1\n",
      "great: 1\n",
      ".: 3\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "lemmatized_words = ['hello', 'hello', 'awesome', 'text', 'great', '.', '.', '.']\n",
    "\n",
    "\n",
    "word_freq = FreqDist(lemmatized_words)\n",
    "for word, freq in word_freq.items():\n",
    "    print(f\"{word}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Removing HTML tags\n",
    "Remove HTML tags from the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Hello World. This is NLTK.\n"
     ]
    }
   ],
   "source": [
    "%pip install bs4\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_text = \"<p>Hello World. This is NLTK.</p>\"\n",
    "soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "cleaned_text = soup.get_text()\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Detecting Language\n",
    "Detect the language of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in /Users/praneet/anaconda3/envs/proj1maverick/lib/python3.9/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /Users/praneet/anaconda3/envs/proj1maverick/lib/python3.9/site-packages (from langdetect) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "en\n"
     ]
    }
   ],
   "source": [
    "%pip install langdetect\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "language = detect(text)\n",
    "print(language) #`en` (for English)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. Tokenizing by Regular Expressions\n",
    "Use Regular Expressions to tokenize text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', 'This', 'is', 'NLTK', 'It', 'is', 'great', 'for', 'text', 'preprocessing']\n"
     ]
    }
   ],
   "source": [
    "text = 'Hello World. This is NLTK. It is great for text preprocessing.'\n",
    "\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "pattern = r'\\w+'\n",
    "regex_tokens = regexp_tokenize(text, pattern)\n",
    "print(regex_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. Remove Frequent Words\n",
    "Removes frequent words (also known as ‚Äúhigh-frequency words‚Äù) from a list of tokens using NLTK, you can use the nltk.FreqDist() function to calculate the frequency of each word and filter out the most common ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without frequent words: ['Natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'I', 'love']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# input text\n",
    "text = \"Natural language processing is a field of AI. I love AI.\"\n",
    "\n",
    "# tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# calculate the frequency of each word\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "\n",
    "# remove the most common words (e.g., the top 10% of words by frequency)\n",
    "filtered_tokens = [token for token in tokens if fdist[token] < fdist.N() * 0.1]\n",
    "\n",
    "print(\"Tokens without frequent words:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. Remove extra whitespace\n",
    "Tokenizes the input string into individual sentences and remove any leading or trailing whitespace from each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello World.', 'This is NLTK.', 'It is great for text preprocessing.']\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "\n",
    "# Text data\n",
    "text = 'Hello World. This is NLTK. It is great for text preprocessing.'\n",
    "\n",
    "# Load the sentence tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = tokenizer.tokenize(text)\n",
    "\n",
    "# Remove extra whitespace from each sentence\n",
    "sentences = [sentence.strip() for sentence in sentences]\n",
    "\n",
    "# Print the tokenized sentences\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion üéâ\n",
    "\n",
    "##### Text preprocessing is a crucial step in natural language processing (NLP) and can significantly impact the performance of your models and applications. With NLTK, we have a powerful toolset that simplifies and streamlines these tasks.\n",
    "##### I hope this guide has provided you with a solid foundation for text preprocessing with NLTK. As you continue your journey in NLP, remember that preprocessing is just the beginning. There are many more exciting and advanced techniques to explore and apply in your projects.\n",
    "\n",
    "##### Happy coding!üíª"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
